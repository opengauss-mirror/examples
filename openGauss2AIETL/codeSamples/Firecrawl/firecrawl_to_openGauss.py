import psycopg2
import numpy as np
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any, Optional
import json
import requests
import time
from urllib.parse import urlparse
import asyncio
import aiohttp

class FirecrawlOpenGaussVectorDB:
    def __init__(self, host: str = "localhost", port: int = 5432,
                 database: str = "postgres", user: str = "your_username",
                 password: str = "your_password",
                 firecrawl_api_key: Optional[str] = None,
                 firecrawl_base_url: str = "https://api.firecrawl.dev"):
        # æ•°æ®åº“é…ç½®
        self.connection_params = {
            "host": host,
            "port": port,
            "database": database,
            "user": user,
            "password": password
        }

        # Firecrawl é…ç½®
        self.firecrawl_api_key = firecrawl_api_key
        self.firecrawl_base_url = firecrawl_base_url.rstrip('/')

        # æ¨¡å‹åˆå§‹åŒ–
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

        # åˆå§‹åŒ–æ•°æ®åº“
        self.init_database()

    def get_connection(self):
        """å»ºç«‹æ•°æ®åº“è¿æ¥"""
        return psycopg2.connect(**self.connection_params)

    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨å’Œå‘é‡æ‰©å±•"""
        try:
            conn = self.get_connection()
            cursor = conn.cursor()

            # cursor.execute("CREATE EXTENSION IF NOT EXISTS vector;")

            create_table_query = """
                                 CREATE TABLE IF NOT EXISTS crawled_documents \
                                 ( \
                                     id \
                                     SERIAL \
                                     PRIMARY \
                                     KEY, \
                                     url \
                                     TEXT \
                                     NOT \
                                     NULL \
                                     UNIQUE, \
                                     title \
                                     TEXT, \
                                     content \
                                     TEXT \
                                     NOT \
                                     NULL, \
                                     metadata \
                                     JSONB, \
                                     embedding \
                                     vector \
                                 ( \
                                     384 \
                                 ),
                                     crawled_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                                     domain TEXT,
                                     word_count INTEGER
                                     ); \
                                 """
            cursor.execute(create_table_query)

            create_index_query = """
                                 CREATE INDEX IF NOT EXISTS crawled_docs_embedding_idx
                                     ON crawled_documents
                                     USING ivfflat (embedding vector_l2_ops)
                                     WITH (lists = 100); \
                                 """
            cursor.execute(create_index_query)

            conn.commit()
            print("Database initialized successfully")

        except Exception as e:
            print(f"Error initializing database: {e}")
        finally:
            if conn:
                cursor.close()
                conn.close()

    def generate_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆæœ¬åœ°æ–‡æœ¬åµŒå…¥"""
        # é™åˆ¶æ–‡æœ¬é•¿åº¦ä»¥é¿å…æ¨¡å‹è¾“å…¥é™åˆ¶
        if len(text) > 512:
            text = text[:512]
        embedding = self.embedding_model.encode(text)
        return embedding.tolist()

    def array_to_vector_string(self, array: List[float]) -> str:
        """å°†æ•°ç»„è½¬æ¢ä¸ºopenGausså‘é‡æ ¼å¼å­—ç¬¦ä¸²"""
        return '[' + ','.join(map(str, array)) + ']'

    async def crawl_url(self, url: str, options: Dict[str, Any] = None) -> Optional[Dict[str, Any]]:
        """ä½¿ç”¨ Firecrawl æŠ“å–å•ä¸ª URL"""
        if not self.firecrawl_api_key:
            raise ValueError("Firecrawl API key is required for crawling")

        default_options = {
            "formats": ["markdown"],
            "onlyMainContent": True,
        }

        final_options = {**default_options, **(options or {})}

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.firecrawl_api_key}"
        }

        payload = {
            "url": url,
            "formats": final_options["formats"],
            "onlyMainContent": final_options["onlyMainContent"],
        }

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                        f"{self.firecrawl_base_url}/v1/scrape",
                        headers=headers,
                        json=payload,
                        timeout=30
                ) as response:

                    if response.status == 200:
                        data = await response.json()
                        print(f"Raw response for {url}: {json.dumps(data, ensure_ascii=False, indent=2)}")

                        if data.get("success"):
                            return data
                        else:
                            print(f"Firecrawl error: {data.get('error')}")
                            return None
                    else:
                        error_text = await response.text()
                        print(f"HTTP error {response.status} for {url}: {error_text}")
                        return None

        except Exception as e:
            print(f"Error crawling {url}: {e}")
            return None

    def process_firecrawl_response(self, response_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """å¤„ç† Firecrawl å“åº”æ•°æ®"""
        try:
            if not response_data.get("success"):
                print(f"Firecrawl request failed: {response_data.get('error')}")
                return None

            data = response_data.get("data", {})

            # è°ƒè¯•ï¼šæ‰“å°å®Œæ•´çš„å“åº”ç»“æ„
            print(f"Processing response data: {json.dumps(data, ensure_ascii=False, indent=2)}")

            # æå–å…³é”®ä¿¡æ¯ - æ ¹æ®å®é™…çš„ Firecrawl å“åº”ç»“æ„è°ƒæ•´
            url = data.get("url", data.get("metadata", {}).get("url", ""))

            # å°è¯•ä¸åŒçš„å†…å®¹å­—æ®µ
            content = ""
            if "markdown" in data:
                content = data["markdown"]
            elif "text" in data:
                content = data["text"]
            elif "html" in data:
                content = data["html"]
            elif "content" in data:
                content = data["content"]

            # æå–æ ‡é¢˜
            title = data.get("title", "")
            if not title and "metadata" in data:
                title = data["metadata"].get("title", "")

            # æå–å…ƒæ•°æ®
            metadata = data.get("metadata", {})
            if not metadata:
                metadata = {
                    "url": url,
                    "title": title,
                    "source": "Firecrawl"
                }

            if not url:
                print("No URL found in response data")
                return None

            if not content:
                print(f"No content found for {url}")
                return None

            return {
                "url": url,
                "title": title,
                "content": content,
                "metadata": metadata
            }

        except Exception as e:
            print(f"Error processing Firecrawl response: {e}")
            return None

    def store_document(self, document_data: Dict[str, Any]) -> bool:
        """å­˜å‚¨æ–‡æ¡£åˆ°æ•°æ®åº“"""
        try:
            url = document_data.get("url")
            title = document_data.get("title", "")
            content = document_data.get("content", "")
            metadata = document_data.get("metadata", {})

            if not url:
                print("Cannot store document: missing URL")
                return False

            if not content:
                print(f"Cannot store document {url}: missing content")
                return False

            # ç”ŸæˆåµŒå…¥
            embedding_text = f"{title} {content}"[:500]  # é™åˆ¶é•¿åº¦
            embedding = self.generate_embedding(embedding_text)

            # æå–åŸŸå
            domain = urlparse(url).netloc

            # è®¡ç®—å­—æ•°
            word_count = len(content.split())

            conn = self.get_connection()
            cursor = conn.cursor()

            insert_query = """
                           INSERT INTO crawled_documents
                               (url, title, content, metadata, embedding, domain, word_count)
                           VALUES (%s, %s, %s, %s, %s::vector, %s, %s)
                           """

            cursor.execute(insert_query, (
                url,
                title,
                content,
                json.dumps(metadata),
                self.array_to_vector_string(embedding),
                domain,
                word_count
            ))

            conn.commit()
            print(f"Successfully stored: {url} ({word_count} words)")
            return True

        except Exception as e:
            print(f"Error storing document {document_data.get('url', 'unknown')}: {e}")
            return False
        finally:
            if conn:
                cursor.close()
                conn.close()

    async def crawl_and_store_url(self, url: str, options: Dict[str, Any] = None) -> bool:
        """å®Œæ•´çš„æŠ“å–å’Œå­˜å‚¨æµç¨‹"""
        print(f"ğŸ•·ï¸  Crawling: {url}")

        # æŠ“å– URL
        response = await self.crawl_url(url, options)
        if not response:
            print(f"âŒ Failed to crawl {url}")
            return False

        # å¤„ç†å“åº”
        document_data = self.process_firecrawl_response(response)
        if not document_data:
            print(f"âŒ Failed to process response for {url}")
            return False

        # å­˜å‚¨åˆ°æ•°æ®åº“
        success = self.store_document(document_data)
        if success:
            print(f"âœ… Successfully processed and stored: {url}")
        else:
            print(f"âŒ Failed to store: {url}")

        return success

    async def crawl_multiple_urls(self, urls: List[str], options: Dict[str, Any] = None) -> List[bool]:
        """æ‰¹é‡æŠ“å–å¤šä¸ª URLs"""
        results = []

        for url in urls:
            success = await self.crawl_and_store_url(url, options)
            results.append(success)
            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            await asyncio.sleep(2)

        return results

    def similarity_search(self, query: str, top_k: int = 5, domain: str = None) -> List[Any]:
        """åŸºäºç›¸ä¼¼æ€§æœç´¢æŠ“å–çš„æ–‡æ¡£"""
        try:
            query_embedding = self.generate_embedding(query)
            conn = self.get_connection()
            cursor = conn.cursor()

            if domain:
                search_query = """
                               SELECT id, \
                                      url, \
                                      title, \
                                      content, \
                                      metadata,
                                      l2_distance(embedding, %s::vector) as distance
                               FROM crawled_documents
                               WHERE domain = %s
                               ORDER BY distance
                                   LIMIT %s; \
                               """
                cursor.execute(search_query, (
                    self.array_to_vector_string(query_embedding),
                    domain,
                    top_k
                ))
            else:
                search_query = """
                               SELECT id, \
                                      url, \
                                      title, \
                                      content, \
                                      metadata,
                                      l2_distance(embedding, %s::vector) as distance
                               FROM crawled_documents
                               ORDER BY distance
                                   LIMIT %s; \
                               """
                cursor.execute(search_query, (
                    self.array_to_vector_string(query_embedding),
                    top_k
                ))

            results = cursor.fetchall()
            return results

        except Exception as e:
            print(f"Error in similarity search: {e}")
            return []
        finally:
            if conn:
                cursor.close()
                conn.close()

    def get_crawl_statistics(self) -> Dict[str, Any]:
        """è·å–æŠ“å–ç»Ÿè®¡ä¿¡æ¯"""
        try:
            conn = self.get_connection()
            cursor = conn.cursor()

            stats = {}

            # æ€»æ–‡æ¡£æ•°
            cursor.execute("SELECT COUNT(*) FROM crawled_documents")
            stats['total_documents'] = cursor.fetchone()[0]

            # æ€»å­—æ•°
            cursor.execute("SELECT SUM(word_count) FROM crawled_documents")
            stats['total_words'] = cursor.fetchone()[0] or 0

            # åŸŸååˆ†å¸ƒ
            cursor.execute("""
                           SELECT domain, COUNT (*) as count, SUM (word_count) as total_words
                           FROM crawled_documents
                           GROUP BY domain
                           ORDER BY count DESC
                           """)
            stats['domain_distribution'] = cursor.fetchall()

            return stats

        except Exception as e:
            print(f"Error getting statistics: {e}")
            return {}
        finally:
            if conn:
                cursor.close()
                conn.close()

    def list_all_documents(self) -> List[Any]:
        """åˆ—å‡ºæ‰€æœ‰å­˜å‚¨çš„æ–‡æ¡£"""
        try:
            conn = self.get_connection()
            cursor = conn.cursor()

            cursor.execute("""
                           SELECT id, url, title, domain, word_count, crawled_at
                           FROM crawled_documents
                           ORDER BY crawled_at DESC
                           """)

            return cursor.fetchall()

        except Exception as e:
            print(f"Error listing documents: {e}")
            return []
        finally:
            if conn:
                cursor.close()
                conn.close()


# ä½¿ç”¨ç¤ºä¾‹
async def demo():
    # åˆå§‹åŒ–ï¼ˆéœ€è¦è®¾ç½®ä½ çš„ Firecrawl API keyï¼‰
    firecrawl_api_key = "fc-xxxxxxxxx"  # æ›¿æ¢ä¸ºä½ çš„å®é™… API key

    if firecrawl_api_key == "your_firecrawl_api_key_here":
        print("âŒ Please set your Firecrawl API key")
        return

    vector_db = FirecrawlOpenGaussVectorDB(
        firecrawl_api_key=firecrawl_api_key
    )

    # è¦æŠ“å–çš„ URLs - ä½¿ç”¨ä¸€äº›å¯é çš„æµ‹è¯•URL
    urls_to_crawl = [
        "https://lilianweng.github.io/posts/2024-07-07-hallucination/",
        "https://lilianweng.github.io/posts/2024-11-28-reward-hacking/",
        "https://lilianweng.github.io/posts/2025-05-01-thinking/"
    ]

    print("Starting web crawling...")

    # æŠ“å–ç½‘é¡µ
    results = await vector_db.crawl_multiple_urls(urls_to_crawl)

    success_count = sum(results)
    print(f"\nCrawling completed: {success_count} successful, {len(results) - success_count} failed")

    # åˆ—å‡ºæ‰€æœ‰å­˜å‚¨çš„æ–‡æ¡£
    print("\nğŸ“‹ Stored documents:")
    documents = vector_db.list_all_documents()
    for doc_id, url, title, domain, word_count, crawled_at in documents:
        print(f"  - {title} ({domain}, {word_count} words)")

    # æ‰§è¡Œç›¸ä¼¼æ€§æœç´¢
    if documents:
        print("\nğŸ” Testing similarity search...")
        queries = [
            "machine learning",
            "artificial intelligence",
            "neural networks"
        ]

        for query in queries:
            print(f"\nSearch query: '{query}'")
            results = vector_db.similarity_search(query, top_k=2)

            if results:
                print("Most relevant results:")
                for i, (doc_id, url, title, content, metadata, distance) in enumerate(results):
                    print(f"{i + 1}. ğŸ“ Distance: {distance:.4f}")
                    print(f"   ğŸ·ï¸  Title: {title}")
                    print(f"   ğŸŒ URL: {url}")
                    print(f"   ğŸ“ Preview: {content[:100]}...")
                    print()
            else:
                print("No results found")

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = vector_db.get_crawl_statistics()
    print(f"\nğŸ“Š Crawl statistics:")
    print(f"   Total documents: {stats.get('total_documents', 0)}")
    print(f"   Total words: {stats.get('total_words', 0)}")
    print("   Domain distribution:")
    for domain, count, total_words in stats.get('domain_distribution', []):
        print(f"     {domain}: {count} documents, {total_words} words")


# æµ‹è¯•å‡½æ•° - å…ˆæµ‹è¯•æ•°æ®å¤„ç†è€Œä¸å®é™…è°ƒç”¨ API
def test_data_processing():
    """æµ‹è¯•æ•°æ®å¤„ç†é€»è¾‘"""
    print("Testing data processing...")

    vector_db = FirecrawlOpenGaussVectorDB(
        firecrawl_api_key="test"  # ä¸éœ€è¦çœŸå® API key æ¥æµ‹è¯•
    )

    # æ¨¡æ‹Ÿ Firecrawl å“åº”æ•°æ®
    sample_response = {
        "success": True,
        "data": {
            "url": "https://example.com/test",
            "title": "Test Page",
            "markdown": "This is test content about machine learning and AI.",
            "metadata": {
                "title": "Test Page",
                "description": "A test page for Firecrawl",
                "source": "Firecrawl"
            }
        }
    }

    # æµ‹è¯•æ•°æ®å¤„ç†
    processed = vector_db.process_firecrawl_response(sample_response)
    print(f"Processed data: {json.dumps(processed, indent=2, ensure_ascii=False)}")

    if processed:
        # æµ‹è¯•å­˜å‚¨
        success = vector_db.store_document(processed)
        print(f"Storage successful: {success}")


if __name__ == "__main__":
    # å…ˆæµ‹è¯•æ•°æ®å¤„ç†
    test_data_processing()

    # ç„¶åè¿è¡Œå®é™…çš„çˆ¬è™«æ¼”ç¤ºï¼ˆå–æ¶ˆæ³¨é‡Šæ¥è¿è¡Œï¼‰
    # asyncio.run(demo())