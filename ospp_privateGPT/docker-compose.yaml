services:

  #-----------------------------------
  #---- Database services -------------
  #-----------------------------------

  # OpenGauss database service
  opengauss:
    image: opengauss/opengauss:latest
    container_name: opengauss-pg
    privileged: true
    environment:
      GS_PASSWORD: 'MyStrongPass$123'
      GS_CLUSTER_NAME: 'opengauss-cluster'
      GAUSSLOG: '/var/lib/opengauss/log'
      GAUSS_WARNING_TYPE: 'WARNING'
    volumes:
      - opengauss_data:/var/lib/opengauss
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "netstat -tlnp | grep :5432"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    profiles:
      - ""

  #-----------------------------------
  #---- Private-GPT services ---------
  #-----------------------------------

  # Private-GPT service for the Ollama CPU and GPU modes
  # This service builds from an external Dockerfile and runs the Ollama mode.
  # private-gpt-ollama:
  #   image: ${PGPT_IMAGE:-zylonai/private-gpt}:${PGPT_TAG:-0.6.2}-ollama  # x-release-please-version
  #   user: root
  #   build:
  #     context: .
  #     dockerfile: Dockerfile.ollama
  #   volumes:
  #     - ./local_data:/home/worker/app/local_data
  #   ports:
  #     - "8002:8001"
  #   environment:
  #     PORT: 8001
  #     PGPT_PROFILES: docker
  #     PGPT_MODE: ollama
  #     PGPT_EMBED_MODE: ollama
  #     PGPT_OLLAMA_API_BASE: http://ollama-cpu-1:11434
  #     HF_TOKEN: ${HF_TOKEN:-}
  #   profiles:
  #     - ""
  #     - ollama-cpu
  #     - ollama-cuda
  #     - ollama-api
  #   depends_on:
  #     ollama-cpu:
  #       condition: service_healthy

  # Private-GPT service built from Ubuntu base image
  # This service builds from Ubuntu 22.04 with Python 3.11 and uses Poetry
  private-gpt-ubuntu:
     build:
       context: .
       dockerfile: Dockerfile.ubuntu
     container_name: private-gpt-ubuntu
     ports:
       - "8001:8001"
     volumes:
       - ./local_data:/app/local_data
       - ./models:/app/models
     environment:
       PGPT_PROFILES: ollama-opengauss
       PORT: 8001
       PGPT_OLLAMA_API_BASE: http://ollama:11434
       PGPT_OPENGAUSS_API_HOST: opengauss-pg
       PGPT_OPENGAUSS_API_PORT: 5432
       PGPT_OPENGAUSS_API_DATABASE: postgres
       PGPT_OPENGAUSS_API_USER: gaussdb
       PGPT_OPENGAUSS_API_PASSWORD: MyStrongPass$123
       PGPT_OPENGAUSS_API_SCHEMA_NAME: private_gpt
     depends_on:
       opengauss:
         condition: service_healthy
       ollama:
         condition: service_healthy
     extra_hosts:
       - "host.docker.internal:host-gateway"
     profiles:
       - ubuntu
       - ""

  # Traefik reverse proxy for the Ollama service
  # This will route requests to the Ollama service based on the profile.
  ollama:
    image: traefik:v2.10
    healthcheck:
      test: ["CMD", "sh", "-c", "wget -q --spider http://ollama:11434 || exit 1"]
      interval: 10s
      retries: 3
      start_period: 5s
      timeout: 5s
    ports:
      - "8080:8080"
    command:
      - "--providers.file.filename=/etc/router.yml"
      - "--log.level=ERROR"
      - "--api.insecure=true"
      - "--providers.docker=true"
      - "--providers.docker.exposedbydefault=false"
      - "--entrypoints.web.address=:11434"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./.docker/router.yml:/etc/router.yml:ro
    extra_hosts:
      - "host.docker.internal:host-gateway"
    profiles:
      - ""
      - ollama-cpu
      - ollama-cuda
      - ollama-api

  # Ollama service for the CPU mode
  ollama-cpu:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ./models:/root/.ollama
    profiles:
      - ""
      - ollama-cpu

  # Ollama service for the CUDA mode
  ollama-cuda:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ./models:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles:
      - ollama-cuda

volumes:
  ollama_models:
  opengauss_data:
